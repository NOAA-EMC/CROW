action_template: &action_template
  <<: *resource_defaults
  BASE_GSI: !calc doc.platform.places.BASE_GSI
  BASE_GSM: !calc doc.platform.places.BASE_GSM

ecen: &ecen_action !Action
  <<: *action_template

  # ----------------------------------------
  # From config.resources
  walltime: !timedelta 00:30:00
  resources: !calc run_ecen
  rocoto_memory: "3072M"

  # Each command (APRUN_whatever) in config.resources needs a
  # run_whatever entry in the corresponding action.
  # Executable name is specified deep inside scripts
  # Use "placeholder" for exe name
  # ----------------------------------------
  # From config.ecen
  ENKFRECENSH: !expand "{BASE_GSI}/scripts/EnKF/scripts_ncep/exglobal_enkf_recenter_fv3gfs.sh.ecf"
  nth_ecen: 2
  CHGRESSH: !expand "{BASE_GSM}/ush/global_chgres_GSM.sh"
  CHGRESEXEC: !expand "{BASE_GSM}/exec/global_chgres_GSM"
  CHGRESVARS_ENKF: "use_ufo=.true.,nopdpvv=.true."
  CHGRESTHREAD: 12
  J_JOB: ecen
  accounting: !calc doc.platform.parallel_accounting

anal: &anal_action !Action
  <<: *action_template
  J_JOB: anal
  walltime: !timedelta 01:30:00
  resources: !calc run_anal
  rocoto_memory: "3072M"
  ANALYSISSH: !expand "{BASE_GSI}/scripts/exglobal_analysis_fv3gfs.sh.ecf"
  GSIEXEC: !expand "{BASE_GSI}/exec/global_gsi"
  npe_gsi: !expand "{npe_anal}"
  nth_gsi: 4
  nth_anal: 2
  accounting: !calc doc.platform.parallel_accounting

epos: &epos_action !Action
  <<: *action_template
  J_JOB: epos
  walltime: !timedelta 00:15:00
  resources: !calc run_epos
  rocoto_memory: "3072M"
  ENKFPOSTSH: !expand "{BASE_GSI}/scripts/EnKF/scripts_ncep/exglobal_enkf_post_fv3gfs.sh.ecf"
  nth_epos: 2
  accounting: !calc doc.platform.parallel_accounting

eobs: &eobs_action !Action
  <<: *anal_action
  J_JOB: eobs
  walltime: !timedelta 00:15:00
  resources: !calc run_eobs
  rocoto_memory: "3072M"
  INVOBSSH: !expand "{BASE_GSI}/scripts/exglobal_innovate_obs_fv3gfs.sh.ecf"
  ENKFINVOBSSH: !expand "{BASE_GSI}/scripts/EnKF/scripts_ncep/exglobal_enkf_innovate_obs_fv3gfs.sh.ecf"
  NMEM_EOMGGRP: 10
  RERUN_EOMGGRP: "YES"
  npe_gsi: !expand "{npe_eobs}"
  nth_gsi: 4
# GSI namelist options related to observer for EnKF
  OBSINPUT_INVOBS: "dmesh(1)=225.0,dmesh(2)=225.0"
  OBSQC_INVOBS: "tcp_width=60.0,tcp_ermin=2.0,tcp_ermax=12.0"
  nth_eobs: 2
  accounting: !calc doc.platform.parallel_accounting

eomg: &eomg_action !Action
  <<: *action_template
  J_JOB: eomg
  walltime: !timedelta 00:15:00
  resources: !calc run_eomg
  rocoto_memory: "3072M"
  nth_eomg: 2
  accounting: !calc doc.platform.parallel_accounting

eupd: &eupd_action !Action
  <<: *action_template
  J_JOB: eupd
  walltime: !timedelta 00:15:00
  resources: !calc run_eupd
  rocoto_memory: "3072M"
  ENKFUPDSH: !expand "{BASE_GSI}/scripts/EnKF/scripts_ncep/exglobal_enkf_update_fv3gfs.sh.ecf"
  ENKFEXEC: !expand "{BASE_GSI}/exec/global_enkf"
  npe_enkf: !expand "{npe_eupd}"
  nth_enkf: 4
  nth_eupd: 2
  accounting: !calc doc.platform.parallel_accounting

efcs: &efcs_action !Action
  <<: *action_template
  J_JOB: efcs
  walltime: !timedelta 00:15:00
  resources: !calc run_efcs
  rocoto_memory: "3072M"
  npe_fv3: !expand "{npe_efcs}"
  nth_fv3: 1
  accounting: !calc doc.platform.parallel_accounting

  ENKFFCSTSH: !expand "{BASE_GSI}/scripts/EnKF/scripts_ncep/exglobal_enkf_fcst_fv3gfs.sh.ecf"
  NMEM_EFCSGRP: 10
  RERUN_EFCSGRP: "NO"

# Stochastic physics parameters (only for ensemble forecasts)
  SET_STP_SEED: "YES"
  DO_SKEB: ".false."
  SKEB: 0.8
  SKEB_TAU: 21600.
  SKEB_LSCALE: 500000.
  SKEBNORM: 1
  DO_SHUM: ".false."
  SHUM: 0.006
  SHUM_TAU: 21600.
  SHUM_LSCALE: 250000.
  DO_SPPT: ".false."
  SPPT: 0.8
  SPPT_TAU: 21600.
  SPPT_LSCALE: 500000.

  DIAG_TABLE: !expand "{BASE_GSM}/parm/parm_fv3diag/diag_table_da"
  restart_interval: 6

  nth_efcs: 2

earc: &earc_action !Action
  <<: *action_template
  J_JOB: earc
  walltime: !timedelta 00:15:00
  resources: !calc run_earc
  rocoto_memory: "3072M"
  NMEM_EARCGRP: 10
  nth_earc: 2
  accounting: !calc doc.platform.transfer_accounting

final: &final_action !Action
  <<: *action_template
  walltime: !timedelta 00:03:00
  resources: !calc run_nothing
  rocoto_memory: "100M"
  accounting: !calc doc.platform.serial_accounting
  J_JOB: /bin/true

prep: &prep_action !Action
  <<: *action_template
  J_JOB: prep
  walltime: !timedelta 00:10:00
  resources: !calc run_prep
  rocoto_memory: "3072M"
  DO_RELOCATE: "NO"
  DO_MAKEPREPBUFR: "YES"   # if NO, will copy prepbufr from globaldump
  DRIVE_MAKEPREPBUFRSH: !expand "{BASE_GSM}/ush/drive_makeprepbufr.sh"
  nth_prep: 2
  accounting: !calc doc.platform.parallel_accounting

fcst: &fcst_action !Action
  <<: *action_template
  J_JOB: fcst
  walltime: !timedelta 00:10:00
  resources: !calc run_fcst
  rocoto_memory: "3072M"
  FORECASTSH: !expand "{BASE_GSM}/scripts/exglobal_fcst_nemsfv3gfs.sh"
  FCSTEXECDIR: !expand "{BASE_NEMSfv3gfs}/NEMS/exe"
  FCSTEXEC: "fv3_gfs_nh.prod.32bit.x"
  npe_fv3: !expand "{npe_fcst}" # This is model resolution dependent, see note above
  nth_fv3: 2
  TYPE: "nh"
  MONO: "non-mono"
  do_vort_damp: ".true."    # vorticity and divergence damping
  consv_te: "0."            # conserve total energy
  fv_sg_adj: 900            # time-scale to remove 2dz instability
  dspheat: ".false."        # dissipative heating
  shal_cnv: ".true."        # shallow convection
  agrid_vel_rst: ".true."   # write velocity restarts on agrid?
  accounting: !calc doc.platform.parallel_accounting

# Disable the use of coupler.res; get model start time from model_configure
# export USE_COUPLER_RES="NO"

  USE_COUPLER_RES: "NO"

  restart_interval: !FirstTrue
    - when: !calc CDUMP=="gdas"
      do: 6
    - otherwise: 0

  DIAG_TABLE: !FirstTrue
    - when: !calc CDUMP=="gdas"
      do: !expand "{BASE_GSM}/parm/parm_fv3diag/diag_table_da"
    - when: !calc CDUMP=="gfs"
      do: !expand "{BASE_GSM}/parm/parm_fv3diag/diag_table"
    - otherwise: !error "Do not know DIAG_TABLE for CDUMP={CDUMP}"

  REGRID_NEMSIO_SH: !expand "{BASE_GSM}/ush/fv3gfs_regrid_nemsio.sh"
  REGRID_NEMSIO_TBL: !expand "{BASE_GSM}/parm/parm_fv3diag/variable_table_da.txt"

  REMAPSH: !expand "{BASE_GSM}/ush/fv3gfs_remap.sh"
  master_grid: "0p5deg" # 1deg 0p5deg 0p25deg 0p125deg etc
  npe_remap: !expand "{npe_fcst}"
  nth_remap: 2
  NC2NEMSIOSH: !expand "{BASE_GSM}/ush/fv3gfs_nc2nemsio.sh"
  nth_fcst: 2

post: &post_action !Action
  <<: *action_template
  J_JOB: post
  walltime: !timedelta 00:15:00
  resources: !calc run_post
  rocoto_memory: "3072M"
  POSTJJOBSH: !expand "{BASE_WORKFLOW}/jobs/JGFS_POST.sh"
  POSTGPSH: !expand "{BASE_POST}/ush/global_nceppost.sh"
  POSTGPEXEC: !expand "{BASE_POST}/exec/ncep_post"
  npe_postgp: !expand "{npe_post}"
  nth_postgp: 1
  GFS_DOWNSTREAM: "YES"
  GFSDOWNSH: !expand "{BASE_WORKFLOW}/ush/fv3gfs_downstream_nems.sh"
  GFSDWNSH: !expand "{BASE_WORKFLOW}/ush/fv3gfs_dwn_nems.sh"
  downset: 1
  npe_dwn: !expand "{npe_post}"
  nth_dwn: 2
  nth_post: 2
  accounting: !calc doc.platform.parallel_accounting

arch: &arch_action !Action
  <<: *action_template
  J_JOB: arch
  walltime: !timedelta 06:00:00
  resources: !calc run_arch
  rocoto_memory: "3072M"
  nth_arch: 2
  accounting: !calc doc.platform.transfer_accounting

vrfy: &vrfy_action !Action
  <<: *action_template
  J_JOB: vrfy
  Template: *vrfy_template
  walltime: !timedelta 01:00:00
  resources: !calc run_vrfy
  rocoto_memory: "3072M"
  accounting: !calc doc.platform.parallel_accounting
#  CDUMP: "gfs"
  VDUMP: "gfs"       # verifying dump
  CDUMPFCST: "gdas"  # Fit-to-obs with GDAS/GFS prepbufr
  CDFNL: "gdas"      # Scores verification against GDAS/GFS analysis
  VSDB_STEP1: "YES"      # populate VSDB database
  VSDB_STEP2: "NO"
  VRFYG2OBS: "YES"       # Grid to observations
  VRFYFITS: "YES"        # Fit to observations
  VRFYPRCP: "YES"        # Precip threat scores
  VRFYMINMON: "YES"      # GSI minimization monitoring
  VRFYRAD: "YES"         # Radiance data assimilation monitoring
  VRFYOZN: "NO"          # Ozone data assimilation monitoring
  VRFYTRAK: "YES"        # Hurricane track forecasts
  VRFYGENESIS: "YES"     # Cyclone genesis
  VRFYGMPK: "NO"         # Gempak verification
  nth_vrfy: 2

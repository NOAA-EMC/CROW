
theia: &theia !Platform
  <<: *resource_defaults
  Evaluate: false
  name: THEIA
  detect: !calc tools.isdir("/scratch4") and tools.isdir("/scratch3")

  BASE_SVN: "/scratch4/NCEPDEV/global/save/glopara/svn"

  # Environment variables to set in all jobs
  general_env: 
    POSTGRB2TBL: "/scratch3/NCEPDEV/nwprod/lib/sorc/g2tmpl/params_grib2_tbl_new"
    NWPROD: "/scratch4/NCEPDEV/global/save/glopara/nwpara"
    DMPDIR: "/scratch4/NCEPDEV/global/noscrub/dump"
    RTMFIX: "/scratch4/NCEPDEV/da/save/Michael.Lueken/nwprod/lib/crtm/2.2.3/fix_update"
    NEMSIOGET: !expand "{doc.places.BASE_GSM}/exec/nemsio_get"
    #NEMSIOGET: !expand "{NWPROD}/util/exec/nemsio_get"
    NDATE: !expand "{NWPROD}/util/exec/ndate"
    NHOUR: !expand "{NWPROD}/util/exec/nhour"
    WGRIB: !expand "{NWPROD}/util/exec/wgrib"
    WGRIB2: !expand "{NWPROD}/util/exec/wgrib2"
    COPYGB: !expand "{NWPROD}/util/exec/copygb"
    COPYGB2: !expand "{NWPROD}/util/exec/copygb2"
    GRBINDEX: !expand "{NWPROD}/util/exec/grbindex"
    GRB2INDEX: !expand "{NWPROD}/util/exec/grb2index"
    GRBINDEX2: !expand "{NWPROD}/util/exec/grb2index"
    CNVGRIB: "/apps/cnvgrib/1.4.0/bin/cnvgrib"
    WGRIB: !expand "{NWPROD}/util/exec/wgrib"
    WGRIB2: "/scratch3/NCEPDEV/nwprod/utils/wgrib2.v2.0.6c/wgrib2/wgrib2"

    prep_step: !expand "{NWPROD}/prod_util.v1.0.15/ush/prep_step"

    NCP: "/bin/cp -p"
    NLN: "/bin/ln -sf"
    NMV: "/bin/mv"

    CHGRP_CMD: chgrp rstprod

  default_cpu_project: fv3-cpu

  serial_accounting:
    queue: debug
    project: !calc doc.accounting.cpu_project
  transfer_accounting:
    queue: service
    project: !calc doc.accounting.cpu_project
  parallel_accounting:
    queue: batch
    project: !calc doc.accounting.cpu_project

  scheduler_settings: &theia_scheduler
    name: MoabTorque
    physical_cores_per_node: 24
    logical_cpus_per_core: 2
    hyperthreading_allowed: true
    indent_text: "  "
  parallelism_settings: { <<: *theia_scheduler, name: HydraIMPI }
  node_type_settings:   { <<: *theia_scheduler, node_type: generic }

  mpi_tuning:
      MPI_BUFS_PER_HOST: 2048
      MPI_BUFS_PER_PROC: 2048
      MPI_GROUP_MAX: 256
      MPI_MEMMAP_OFF: 1
      MP_STDOUTMODE: "ORDERED"
      NTHSTACK: 1024000000
      OMP_STACKSIZE: 2048000

  scheduler: !calc |
    tools.get_scheduler(scheduler_settings.name, scheduler_settings)
  parallelism: !calc |
    tools.get_parallelism(parallelism_settings.name, parallelism_settings)
  nodes: !calc |
    tools.node_tool_for(node_type_settings.node_type, node_type_settings)

  # Path to pan_df, the program used to get Panasas disk usage information:
  pan_df: pan_df
  least_used_scrub: !Immediate 
    - !FirstMax
      - do: /scratch3/NCEPDEV/stmp1
        when: !calc tools.panasas_gb(do)
      - do: /scratch3/NCEPDEV/stmp2
        when: !calc tools.panasas_gb(do)
      - do: /scratch4/NCEPDEV/stmp3
        when: !calc tools.panasas_gb(do)
      - do: /scratch4/NCEPDEV/stmp4
        when: !calc tools.panasas_gb(do)
